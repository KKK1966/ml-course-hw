{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Credits: materials from this notebook belong to YSDA [Practical DL](https://github.com/yandexdataschool/Practical_DL) course. Special thanks for making them available online.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab assignment №1, part 1\n",
    "\n",
    "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
    "\n",
    "Several comments:\n",
    "* Don't hesitate to ask questions, it's a good practice.\n",
    "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
    "* Blocks of this lab will be graded separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matrix differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it easy to google every task please please please try to undestand what's going on. The \"just answer\" thing will be not counted, make sure to present derivation of your solution. It is absolutely OK if you found an answer on web then just exercise in $\\LaTeX$ copying it into here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links: \n",
    "[1](http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf)\n",
    "[2](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \n",
    "y = x^Tx,  \\quad x \\in \\mathbb{R}^N \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial }{\\partial x_j} \\sum_i (x_i^2) = \\sum_i \\frac{\\partial }{\\partial x_j}  (x_i^2) = 2x_j \n",
    "$$\n",
    "$$\n",
    " \\nabla y = 2x\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = tr(AB) \\quad A,B \\in \\mathbb{R}^{N \\times N} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$  y = \\sum_{i=1}^N \\sum_{j=1}^N (a_{ij}b_{ji})  $$\n",
    "$$ \\frac{\\partial y}{\\partial a_{ij}} = b_{ji} \\Longrightarrow \\frac{dy}{dA} = B^T\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \n",
    "y = x^TAc , \\quad A\\in \\mathbb{R}^{N \\times N}, x\\in \\mathbb{R}^{N}, c\\in \\mathbb{R}^{N} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac {\\partial y}{\\partial x_i} = \\frac {\\partial}{\\partial x_i} (\\sum_{j=1}^N {x_j} \\sum_{k=1}^N a_{jk}c_k) = \\sum_{j=1}^N \\frac {\\partial}{\\partial x_i}  ({x_j} \\sum_{k=1}^N a_{jk}c_k) = \\sum_{k=1}^N a_{ik}c_k\n",
    "$$\n",
    "$$\n",
    "\\nabla y = c^TA^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac {\\partial y}{\\partial a_{ij}} = \\frac {\\partial}{\\partial a_{ij}} (\\sum_{j=1}^N {x_j} \\sum_{k=1}^N a_{jk}c_k) = \\sum_{j=1}^N   \\frac {\\partial}{\\partial a_{ij}}({x_j} \\sum_{k=1}^N (a_{jk}c_k)) =  x_i c_j\n",
    "$$\n",
    "$$\n",
    "\\frac{dy}{dA} = xc^T\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint for the latter (one of the ways): use *ex. 2* result and the fact \n",
    "$$\n",
    "tr(ABC) = tr (CAB)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic matrix factorization example. Given matrix $X$ you need to find $A$, $S$ to approximate $X$. This can be done by simple gradient descent iteratively alternating $A$ and $S$ updates.\n",
    "$$\n",
    "J = || X - AS ||_F^2  , \\quad A\\in \\mathbb{R}^{N \\times R} , \\quad S\\in \\mathbb{R}^{R \\times M}\n",
    "$$\n",
    "$$\n",
    "J = tr((X - AS)(X - AS)^T) = tr((X - AS)(X^T - S^TA^T)) = \n",
    "$$\n",
    "$$\n",
    "tr(XX^T - ASX^T -XS^TA^T+ASS^TA^T) = \n",
    "$$\n",
    "\n",
    "$$\n",
    "tr(XX^T) - 2tr(ASX^T) +tr(ASS^TA^T) = tr(XX^T) - 2tr(SX^TA) +tr(S^TA^TAS)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dJ}{dS} = \\frac{d}{dS}(tr(XX^T) - 2tr(SX^TA) +tr(S^TA^TAS)) = -2\\frac{d}{dS}tr(SX^TA) + \\frac{d}{dS}tr(S^TA^TAS) = \n",
    "$$\n",
    "$$\n",
    "= -2(X^TA)^T + 2A^TAS = -2A^TX + 2A^TAS = -2A^T(X - AS)\n",
    "$$\n",
    "$$\\frac{dJ}{dS} = -2A^T(X - AS)$$\n",
    "\n",
    "You may use one of the following approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First approach\n",
    "Using ex.2 and the fact:\n",
    "$$\n",
    "|| X ||_F^2 = tr(XX^T) \n",
    "$$ \n",
    "it is easy to derive gradients (you can find it in one of the refs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second approach\n",
    "You can use *slightly different techniques* if they suits you. Take a look at this derivation:\n",
    "<img src=\"grad.png\">\n",
    "(excerpt from [Handbook of blind source separation, Jutten, page 517](https://books.google.ru/books?id=PTbj03bYH6kC&printsec=frontcover&dq=Handbook+of+Blind+Source+Separation&hl=en&sa=X&ved=0ahUKEwi-q_apiJDLAhULvXIKHVXJDWcQ6AEIHDAA#v=onepage&q=Handbook%20of%20Blind%20Source%20Separation&f=false), open for better picture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third approach\n",
    "And finally we can use chain rule! \n",
    "let $ F = AS $ \n",
    "\n",
    "**Find**\n",
    "$$\n",
    "\\frac{dJ}{dF} =  \n",
    "$$ \n",
    "and \n",
    "$$\n",
    "\\frac{dF}{dS} =  \n",
    "$$ \n",
    "(the shape should be $ NM \\times RM )$.\n",
    "\n",
    "Now it is easy do get desired gradients:\n",
    "$$\n",
    "\\frac{dJ}{dS} =  \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 2. kNN questions\n",
    "Here come the questions from the assignment0_01. Please, refer to the assignment0_01 to get the context of the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Notice the structured patterns in the distance matrix, where some rows or columns are visible brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.)\n",
    "\n",
    "- What in the data is the cause behind the distinctly bright rows?\n",
    "- What causes the columns?\n",
    "\n",
    "*Your Answer:*\n",
    "\n",
    "Матрица расстояний - это матрица, где $a_{ij}$ это расстояние между i-м элементом тестовой выборки и i-v элементом тренинговой выборки. \n",
    "\n",
    "Тогда светлая строка означает что i-й  элемент расположен далеко от всех элементов тренинговой выборки. И скорее всего достоверность его распознавания будет низкой.\n",
    "\n",
    "Если в этой матрице есть светлый столбец, то j-й элемент тренинговой выборки отстоит далеко от всех элементов тестовой выборки и его использования для классификации не целесообразно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "We can also use other distance metrics such as L1 distance.\n",
    "For pixel values $p_{ij}^{(k)}$ at location $(i,j)$ of some image $I_k$, \n",
    "\n",
    "the mean $\\mu$ across all pixels over all images is $$\\mu=\\frac{1}{nhw}\\sum_{k=1}^n\\sum_{i=1}^{h}\\sum_{j=1}^{w}p_{ij}^{(k)}$$\n",
    "And the pixel-wise mean $\\mu_{ij}$ across all images is \n",
    "$$\\mu_{ij}=\\frac{1}{n}\\sum_{k=1}^np_{ij}^{(k)}.$$\n",
    "The general standard deviation $\\sigma$ and pixel-wise standard deviation $\\sigma_{ij}$ is defined similarly.\n",
    "\n",
    "Which of the following preprocessing steps will not change the performance of a Nearest Neighbor classifier that uses L1 distance? Select all that apply.\n",
    "1. Subtracting the mean $\\mu$ ($\\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\\mu$.)\n",
    "2. Subtracting the per pixel mean $\\mu_{ij}$  ($\\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\\mu_{ij}$.)\n",
    "3. Subtracting the mean $\\mu$ and dividing by the standard deviation $\\sigma$.\n",
    "4. Subtracting the pixel-wise mean $\\mu_{ij}$ and dividing by the pixel-wise standard deviation $\\sigma_{ij}$.\n",
    "5. Rotating the coordinate axes of the data.\n",
    "\n",
    "*Your Answer:*\n",
    "Я понимаю этот вопрос так: При каком препроцессинге точность алгоритма kNN не изменится? Тогда ответ такой:\n",
    "\n",
    "1. верно\n",
    "2. верно\n",
    "3. верно\n",
    "4. верно\n",
    "5. неверно\n",
    "\n",
    "*Your Explanation:*\n",
    "\n",
    "L1 при определении расстояния между изображениями выражается так\n",
    "$$\n",
    "\\|p^{(k)} - p^{(l)}\\|_1 = \\sum_{i,j}|p^{(k)}_{ij} - p^{(l)}_{ij}|\n",
    "$$\n",
    "1. В этом случае если значения всех пикселей сдвинуть на одну и ту же величину, то расстояние между отдельными пикселями не изменится и значит вся сумма (норма L1) не изменится\n",
    "$$\n",
    "|p^{(k)}_{ij} - \\mu - (p^{(l)}_{ij} - \\mu)|= |p^{(k)}_{ij}  - p^{(l)}_{ij}|\n",
    "$$\n",
    "2. Если все расстояния между пикселями сдвинуть и нормировать на одинаковую величину $\\sigma$, то абсолютное расстояние изменится, но отношение порядка  ($|x - y| < |y -z| \\Longrightarrow |\\tilde x - \\tilde y| < |\\tilde y - \\tilde z| $) сохранится. т к оно сохранится для каждого пикселя, то сохранится и для суммы в целом, а значит мы также определим ближайших соседей. Но т.к. абсолютное расстояние между пикселями изменится то при использовании взвешенного kNN возможно изменение порядка следования соседей в зависимости от алгоритма взвешивания.\n",
    "3. Мы измерям расстояние между изображениями попиксельно значит сдвиг одинаковых пикселей на одну и ту же величину не изменит расстояние между ними, следовательно и расстояние между изображениями сохранится.\n",
    "$$\n",
    "|p^{(k)}_{ij} - \\mu_{ij} - (p^{(l)}_{ij} - \\mu_{ij})|= |p^{(k)}_{ij}  - p^{(l)}_{ij}|\n",
    "$$\n",
    "4. При такой нормировке отношение порядка сохранится ($|x - y| < |y -z| \\Longrightarrow |\\tilde x - \\tilde y| < |\\tilde y - \\tilde z| $). т к оно сохранится для каждого пикселя, то сохранится и для суммы в целом, а значит мы верно определим ближайших соседей. как и в п.2 Но т.к. абсолютное расстояние между пикселями изменится то при использовании взвешенного kNN возможно изменение порядка следования соседей в зависимости от алгоритма взвешивания.\n",
    "5. Если все изображения поворачиваются на одинаковый угол, то все расстояния сохраняются. Если же изображения поворачиваются на разные углы, то расстояния искажаются. Например если рассмотреть поворот на 180 градусов то в тесте MNIST цифры 6 и 9 совпадут.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Which of the following statements about $k$-Nearest Neighbor ($k$-NN) are true in a classification setting, and for all $k$? Select all that apply.\n",
    "1. The decision boundary (hyperplane between classes in feature space) of the k-NN classifier is linear.\n",
    "2. The training error of a 1-NN will always be lower than that of 5-NN.\n",
    "3. The test error of a 1-NN will always be lower than that of a 5-NN.\n",
    "4. The time needed to classify a test example with the k-NN classifier grows with the size of the training set.\n",
    "5. None of the above.\n",
    "\n",
    "*Your Answer:*\n",
    "1. неверно\n",
    "2. верно\n",
    "3. неверно\n",
    "4. верно\n",
    "\n",
    "\n",
    "*Your Explanation:*\n",
    "1. В k-NN граница практически всегда нелинейна и зависит от распределения в пространстве $\\mathbb R^n$ тренинговой выборки\n",
    "2. training error - это когда мы ищем ближайших соседей сравнивая расстояния между элементами тренинговой  выборки с самой тренинговой выборкой. В  случае 1-NN мы будем проверять дистанцию от элемента до самого себя. Это расстояние всегда равно нулю. Значит и ошибка будет нулевая. А если количество соседей будет больше 1 то возможны неверные определения класса элемента и ошибка может быть больше нуля.\n",
    "3. Можно построить множество в котором один ближайший сосед принадлежит к чужому классу, а из пяти ближайших соседей 4 принадлежат к родственному классу. Таким образом ошибка на 1-NN будет 100%, а на 5-NN - 0\n",
    "4. В k-NN нам необходимо построить матрицу расстояний $M \\times N$ где $M$ - размер тренинговой выборки а $N$ - размер тестовой. Значит при увеличении размера тренинговой выборки размер этой матрицы расстояний возрастет прямо пропорционально.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mipt",
   "language": "python",
   "name": "mipt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
